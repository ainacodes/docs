---
title: Memory
---

AI applications need [memory](/oss/javascript/concepts/memory) to share context across multiple interactions. In LangGraph, you can add two types of memory:

* [**Checkpointers**](#checkpointers) - Thread-scoped persistence for multi-turn conversations and workflow state.
* [**Stores**](#stores) - Cross-thread persistence for user-specific or application-level data.

---

## Checkpointers

Checkpointers provide thread-level [persistence](/oss/javascript/langgraph/persistence), enabling your graph to:
- Track multi-turn conversations
- Resume after interruptions or failures ([durable execution](/oss/javascript/langgraph/durable-execution))
- Access historical states for debugging and [time travel](/oss/javascript/langgraph/use-time-travel)
- Enable [human-in-the-loop](/oss/javascript/langgraph/interrupts) workflows

### Add checkpointers

To add checkpointers to your graph:



```typescript
import { MemorySaver, StateGraph } from "@langchain/langgraph";

const checkpointer = new MemorySaver();

const builder = new StateGraph(...);
const graph = builder.compile({ checkpointer });

await graph.invoke(
  { messages: [{ role: "user", content: "hi! i am Bob" }] },
  { configurable: { thread_id: "1" } }
);
```


<Info>
**What is a thread?**

A thread is a unique conversation or workflow session identified by a `thread_id`. When you invoke your graph with a specific `thread_id`, LangGraph saves checkpoints (snapshots of the graph state) to that thread. All subsequent invocations with the same `thread_id` can access and continue from the saved state.
</Info>

#### Use in production

In production, use a checkpointer backed by a database:



```typescript
import { PostgresSaver } from "@langchain/langgraph-checkpoint-postgres";

const DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable";
const checkpointer = PostgresSaver.fromConnString(DB_URI);

const builder = new StateGraph(...);
const graph = builder.compile({ checkpointer });
```


<Accordion title="Example: using Postgres checkpointer">


  ```
  npm install @langchain/langgraph-checkpoint-postgres
  ```

    <Tip>
    You need to call `checkpointer.setup()` the first time you're using Postgres checkpointer
    </Tip>

  ```typescript
  import { ChatAnthropic } from "@langchain/anthropic";
  import { StateGraph, MessagesZodMeta, START } from "@langchain/langgraph";
  import { BaseMessage } from "@langchain/core/messages";
  import { registry } from "@langchain/langgraph/zod";
  import * as z from "zod";
  import { PostgresSaver } from "@langchain/langgraph-checkpoint-postgres";

  const MessagesZodState = z.object({
    messages: z
      .array(z.custom<BaseMessage>())
      .register(registry, MessagesZodMeta),
  });

  const model = new ChatAnthropic({ model: "claude-haiku-4-5-20251001" });

  const DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable";
  const checkpointer = PostgresSaver.fromConnString(DB_URI);
  // await checkpointer.setup();

  const builder = new StateGraph(MessagesZodState)
    .addNode("call_model", async (state) => {
      const response = await model.invoke(state.messages);
      return { messages: [response] };
    })
    .addEdge(START, "call_model");

  const graph = builder.compile({ checkpointer });

  const config = {
    configurable: {
      thread_id: "1"
    }
  };

  for await (const chunk of await graph.stream(
    { messages: [{ role: "user", content: "hi! I'm bob" }] },
    { ...config, streamMode: "values" }
  )) {
    console.log(chunk.messages.at(-1)?.content);
  }

  for await (const chunk of await graph.stream(
    { messages: [{ role: "user", content: "what's my name?" }] },
    { ...config, streamMode: "values" }
  )) {
    console.log(chunk.messages.at(-1)?.content);
  }
  ```

</Accordion>



### Checkpointers in subgraphs

When your graph contains [subgraphs](/oss/javascript/langgraph/use-subgraphs), understanding how checkpointing works is critical for building multi-agent systems and complex workflows. The behavior depends on which checkpointer option you use when compiling the subgraph.

<Info>
**LangGraph API handles subgraph checkpointing automatically**

When using the LangGraph API, checkpointing is configured automatically for subgraphs. The information below is relevant when self-hosting or when you need to understand the behavior.
</Info>

#### Checkpointer options

When compiling a subgraph, you can control how its state is persisted:



```typescript
// Option 1: No persistent state across invocations (default)
const subgraph = subgraphBuilder.compile();

// Option 2: Persistent state across invocations
const subgraph = subgraphBuilder.compile({ checkpointer: true });

// Option 3: No checkpointing at all
const subgraph = subgraphBuilder.compile({ checkpointer: false });

// Alternative: Use a separate checkpointer instance
import { MemorySaver } from "@langchain/langgraph";
const subgraph = subgraphBuilder.compile({ checkpointer: new MemorySaver() });
```


The most common options are `checkpointer=None` (default) and `checkpointer=True`, which both use the parent's checkpointer but differ in how they namespace the subgraph's state. You can also pass any `BaseCheckpointSaver` instance to give the subgraph its own separate checkpointer, though the common practice for leveraging functionality like `interrupt()` in subgraphs is to use the parent's checkpointer with either `checkpointer=True` or the default `checkpointer=None`.

<Tabs>
  <Tab title="checkpointer=None (default)">
    With the default option, the subgraph uses a dynamic namespace that changes with each invocation. This means the subgraph state resets between runs - each invocation starts with a clean slate. The state from each run is still stored and accessible by the parent graph, but the subgraph itself doesn't carry over state from previous invocations.

    Use this when your subgraph acts as a reusable tool that may be called multiple times and each invocation should start fresh, such as a search tool or data processing pipeline.


    ```typescript expandable
    import { StateGraph, MemorySaver } from "@langchain/langgraph";
    import { registry } from "@langchain/langgraph/zod";
    import * as z from "zod";

    const checkpointer = new MemorySaver();

    const State = z.object({
    parent: z.number().register(registry, {
        reducer: { fn: (x, y) => x + y },
        default: () => 0,
    }),
    });

    const SubgraphState = z.object({
    subgraph: z.number().register(registry, {
        reducer: { fn: (x, y) => x + y },
        default: () => 0,
    }),
    });

    const nodeA = (state: z.infer<typeof State>) => ({ parent: 1 });
    const nodeB = (state: z.infer<typeof SubgraphState>) => ({ subgraph: 1 });

    // Subgraph without persistence
    const subgraphBuilder = new StateGraph(SubgraphState)
    .addNode("nodeB", nodeB)
    .addEntrypoint("nodeB");
    const subgraph = subgraphBuilder.compile();  // No checkpointer

    // Parent graph
    const builder = new StateGraph(State)
    .addNode("nodeA", nodeA)
    .addNode("subgraph", subgraph)
    .addEntrypoint("nodeA")
    .addEdge("nodeA", "subgraph");
    const graph = builder.compile({ checkpointer });

    const config = { configurable: { thread_id: "1" } };

    // Run 1
    for await (const chunk of await graph.stream({}, { ...config, subgraphs: true, streamMode: "values" })) {
    console.log(chunk);
    }
    // Output:
    // [[], { parent: 1 }]
    // [['subgraph:ac5cc169-30a2-7d6e-ae2a-00800959aeb8'], { subgraph: 1 }]

    // Run 2
    for await (const chunk of await graph.stream({}, { ...config, subgraphs: true, streamMode: "values" })) {
    console.log(chunk);
    }
    // Output:
    // [[], { parent: 2 }]  // parent accumulated
    // [['subgraph:b84c6daf-b464-1756-8b39-30d38e101fcf'], { subgraph: 1 }]  // subgraph reset
    ```

    Notice the subgraph namespace changes each run, and `subgraph` value resets to `1` each time.

  </Tab>

  <Tab title="checkpointer=True">
    When you use `checkpointer=True`, the subgraph uses a static namespace that stays the same across invocations. This means the subgraph state persists across runs within the same thread. The subgraph resumes from its last execution, replaying from the last stored checkpoint. Note that this is not time-linear - it replays from the last checkpoint in the namespace, regardless of when that execution occurred.

    Use this for multi-agent systems where each agent should maintain its own conversation history, or when building workflows where subgraphs need to remember things across multiple invocations.


    ```typescript expandable
    import { StateGraph, MemorySaver } from "@langchain/langgraph";
    import { registry } from "@langchain/langgraph/zod";
    import * as z from "zod";

    const checkpointer = new MemorySaver();

    const State = z.object({
    parent: z.number().register(registry, {
        reducer: { fn: (x, y) => x + y },
        default: () => 0,
    }),
    });

    const SubgraphState = z.object({
    subgraph: z.number().register(registry, {
        reducer: { fn: (x, y) => x + y },
        default: () => 0,
    }),
    });

    const nodeA = (state: z.infer<typeof State>) => ({ parent: 1 });
    const nodeB = (state: z.infer<typeof SubgraphState>) => ({ subgraph: 1 });

    // Subgraph with persistent state
    const subgraphBuilder = new StateGraph(SubgraphState)
    .addNode("nodeB", nodeB)
    .addEntrypoint("nodeB");
    const subgraph = subgraphBuilder.compile({ checkpointer: true });  // State persists!

    // Parent graph
    const builder = new StateGraph(State)
    .addNode("nodeA", nodeA)
    .addNode("subgraph", subgraph)
    .addEntrypoint("nodeA")
    .addEdge("nodeA", "subgraph");
    const graph = builder.compile({ checkpointer });

    const config = { configurable: { thread_id: "1" } };

    // Run 1
    for await (const chunk of await graph.stream({}, { ...config, subgraphs: true, streamMode: "values" })) {
    console.log(chunk);
    }
    // Output:
    // [[], { parent: 1 }]
    // [['subgraph'], { subgraph: 1 }]

    // Run 2
    for await (const chunk of await graph.stream({}, { ...config, subgraphs: true, streamMode: "values" })) {
    console.log(chunk);
    }
    // Output:
    // [[], { parent: 2 }]  // parent accumulated
    // [['subgraph'], { subgraph: 2 }]  // subgraph accumulated!
    ```

    Notice the subgraph namespace stays stable, and the `subgraph` value accumulates across runs.

  </Tab>

  <Tab title="checkpointer=False">
    When you use `checkpointer=False`, the subgraph does not use checkpointing at all. No state is saved anywhere, even if the parent graph has a checkpointer configured.

    Use this when the subgraph is purely stateless and you want to avoid any persistence overhead.
  </Tab>
</Tabs>

<Tip>
If your subgraph is used like a function that might be called multiple times (e.g., a tool in a ReAct agent), use `checkpointer=None`. If your subgraph represents an entity with its own memory (e.g., an agent with conversation history), use `checkpointer=True`.
</Tip>

### Access state

With checkpointing enabled, you can access the current state, historical states, and subgraph states.

#### Get current state

You can retrieve the current state of your graph at any time, including the state values and the next nodes that will execute.



```typescript
const config = { configurable: { thread_id: "1" } };
const state = await graph.getState(config);

console.log(state.values);  // Current state values
console.log(state.next);    // Next nodes to execute
```


#### Get state history

You can access the full execution history for a thread to see how the state evolved over time. The history is ordered chronologically from most recent to oldest.



```typescript
const config = { configurable: { thread_id: "1" } };
const history = [];
for await (const state of graph.getStateHistory(config)) {
  history.push(state);
}

// History is ordered from most recent to oldest
for (const state of history) {
  console.log(`Step: ${state.metadata.step}, Values: ${state.values}`);
}
```


<Accordion title="View full state snapshot structure">


  ```javascript
  {
    values: { messages: [HumanMessage(...), AIMessage(...)] },
    next: [],
    config: {
      configurable: {
        thread_id: '1',
        checkpoint_ns: '',
        checkpoint_id: '1ef663ba-28fe-6528-8002-5a559208592c'
      }
    },
    metadata: {
      source: 'loop',
      writes: { call_model: { messages: AIMessage(...) } },
      step: 2
    },
    createdAt: '2024-08-29T19:19:38.821749+00:00',
    parentConfig: {
      configurable: {
        thread_id: '1',
        checkpoint_ns: '',
        checkpoint_id: '1ef663ba-28f9-6ec4-8001-31981c2c39f8'
      }
    },
    tasks: []
  }
  ```

</Accordion>

#### Get subgraph state

To access subgraph state, use `subgraphs=True`:



```typescript
const config = { configurable: { thread_id: "1" } };
const state = await graph.getState(config, { subgraphs: true });

// Access subgraph state from tasks
for (const task of state.tasks) {
  if (task.state) {
    console.log(`Subgraph namespace: ${task.state.config.configurable.checkpoint_ns}`);
    console.log(`Subgraph values: ${task.state.values}`);
  }
}
```


#### Get subgraph state history



To get a subgraph's state history, first get the subgraph's `checkpoint_ns` from `getState({ subgraphs: true })`, then call `getStateHistory` with that namespace:

```typescript
// Step 1: Get the subgraph's checkpoint namespace
const config = { configurable: { thread_id: "1" } };
const state = await graph.getState(config, { subgraphs: true });
const subgraphConfig = state.tasks[0].state.config;
const subgraphNs = subgraphConfig.configurable.checkpoint_ns;

// Step 2: Get subgraph history using that namespace
const subgraphHistory = [];
for await (const h of graph.getStateHistory({
  configurable: {
    thread_id: "1",
    checkpoint_ns: subgraphNs
  }
})) {
  subgraphHistory.push(h);
}

for (const h of subgraphHistory) {
  console.log(`Subgraph step: ${h.metadata.step}, Values: ${h.values}`);
}
```

<Info>
**Why doesn't `getStateHistory()` have a `subgraphs: true` option?**

`getStateHistory()` returns an Iterator to avoid loading all checkpoints into memory at once. Adding `subgraphs: true` would require loading all checkpoints from all nested subgraphs simultaneously, which could cause huge memory overhead. Instead, you explicitly get each subgraph's namespace and fetch its history separately.
</Info>


<Accordion title="Full example: accessing subgraph state and history">


  ```typescript
  import { StateGraph, MemorySaver } from "@langchain/langgraph";
  import { interrupt } from "@langchain/langgraph";
  import * as z from "zod";

  const checkpointer = new MemorySaver();

  const State = z.object({ step: z.string() });

  const nodeA = (state: z.infer<typeof State>) => ({ step: "a" });
  const nodeB = (state: z.infer<typeof State>) => ({ step: "b" });
  const nodeC = (state: z.infer<typeof State>) => ({ step: "c" });
  const nodeD = (state: z.infer<typeof State>) => {
    interrupt("");
    return { step: "d" };
  };

  // Build subgraph
  const subgraphBuilder = new StateGraph(State)
    .addNode("nodeB", nodeB)
    .addNode("nodeC", nodeC)
    .addNode("nodeD", nodeD)
    .addEntrypoint("nodeB")
    .addEdge("nodeB", "nodeC")
    .addEdge("nodeC", "nodeD");
  const subgraph = subgraphBuilder.compile();

  // Build parent graph
  const builder = new StateGraph(State)
    .addNode("nodeA", nodeA)
    .addNode("subgraph", subgraph)
    .addEntrypoint("nodeA")
    .addEdge("nodeA", "subgraph");
  const graph = builder.compile({ checkpointer });

  // Run the graph
  const config = { configurable: { thread_id: "1" } };
  await graph.invoke({ step: "START" }, config);

  // Get subgraph state and history
  const state = await graph.getState(config, { subgraphs: true });
  const subgraphConfig = state.tasks[0].state.config;
  const subgraphNs = subgraphConfig.configurable.checkpoint_ns;
  console.log(`Subgraph namespace: ${subgraphNs}\n`);

  const stateHistory = [];
  for await (const h of graph.getStateHistory({
    configurable: {
      checkpoint_ns: subgraphNs,
      thread_id: "1"
    }
  })) {
    stateHistory.push(h);
  }

  for (const h of stateHistory) {
    console.log(`Step: ${h.metadata.step}, Values: ${h.values}`);
  }
  ```

</Accordion>

### Manage checkpointers

With checkpointing enabled, long conversations can exceed the LLM's context window. Common solutions are:

* [Trim messages](#trim-messages) - Remove first or last N messages (before calling LLM)
* [Delete messages](#delete-messages) - Remove messages from LangGraph state permanently
* [Summarize messages](#summarize-messages) - Summarize earlier messages and replace them with a summary
* [Manage checkpoints](#manage-checkpoints) - Store and retrieve message history
* Custom strategies (e.g., message filtering)

#### Trim messages


Most LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `maxTokens`) to use for handling the boundary.




To trim message history, use the [`trimMessages`](https://js.langchain.com/docs/how_to/trim_messages/) function:

```typescript
import { trimMessages } from "@langchain/core/messages";

const callModel = async (state: z.infer<typeof MessagesZodState>) => {
  const messages = trimMessages(state.messages, {
    strategy: "last",
    maxTokens: 128,
    startOn: "human",
    endOn: ["human", "tool"],
  });
  const response = await model.invoke(messages);
  return { messages: [response] };
};

const builder = new StateGraph(MessagesZodState)
  .addNode("call_model", callModel);
// ...
```




```typescript expandable
import { trimMessages, BaseMessage } from "@langchain/core/messages";
import { ChatAnthropic } from "@langchain/anthropic";
import { StateGraph, START, MessagesZodMeta, MemorySaver } from "@langchain/langgraph";
import { registry } from "@langchain/langgraph/zod";
import * as z from "zod";

const MessagesZodState = z.object({
  messages: z
    .array(z.custom<BaseMessage>())
    .register(registry, MessagesZodMeta),
});

const model = new ChatAnthropic({ model: "claude-3-5-sonnet-20241022" });

const callModel = async (state: z.infer<typeof MessagesZodState>) => {
  const messages = trimMessages(state.messages, {
    strategy: "last",
    maxTokens: 128,
    startOn: "human",
    endOn: ["human", "tool"],
    tokenCounter: model,
  });
  const response = await model.invoke(messages);
  return { messages: [response] };
};

const checkpointer = new MemorySaver();
const builder = new StateGraph(MessagesZodState)
  .addNode("call_model", callModel)
  .addEdge(START, "call_model");
const graph = builder.compile({ checkpointer });

const config = { configurable: { thread_id: "1" } };
await graph.invoke({ messages: [{ role: "user", content: "hi, my name is bob" }] }, config);
await graph.invoke({ messages: [{ role: "user", content: "write a short poem about cats" }] }, config);
await graph.invoke({ messages: [{ role: "user", content: "now do the same but for dogs" }] }, config);
const finalResponse = await graph.invoke({ messages: [{ role: "user", content: "what's my name?" }] }, config);

console.log(finalResponse.messages.at(-1)?.content);
```

```
Your name is Bob, as you mentioned when you first introduced yourself.
```


#### Delete messages

You can delete messages from the graph state to manage the message history.



To delete messages from the graph state, use `RemoveMessage`. For `RemoveMessage` to work, you need to use a state key with `messagesStateReducer` [reducer](/oss/javascript/langgraph/graph-api#reducers), like `MessagesZodState`.

To remove specific messages:

```typescript
import { RemoveMessage } from "@langchain/core/messages";

const deleteMessages = (state) => {
  const messages = state.messages;
  if (messages.length > 2) {
    // remove the earliest two messages
    return {
      messages: messages
        .slice(0, 2)
        .map((m) => new RemoveMessage({ id: m.id })),
    };
  }
};
```


<Warning>
When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:

* Some providers expect message history to start with a `user` message
* Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.
</Warning>

<Accordion title="Full example: delete messages">


  ```typescript
  import { RemoveMessage, BaseMessage } from "@langchain/core/messages";
  import { ChatAnthropic } from "@langchain/anthropic";
  import { StateGraph, START, MemorySaver, MessagesZodMeta } from "@langchain/langgraph";
  import * as z from "zod";
  import { registry } from "@langchain/langgraph/zod";

  const MessagesZodState = z.object({
    messages: z
      .array(z.custom<BaseMessage>())
      .register(registry, MessagesZodMeta),
  });

  const model = new ChatAnthropic({ model: "claude-3-5-sonnet-20241022" });

  const deleteMessages = (state: z.infer<typeof MessagesZodState>) => {
    const messages = state.messages;
    if (messages.length > 2) {
      return { messages: messages.slice(0, 2).map(m => new RemoveMessage({ id: m.id })) };
    }
    return {};
  };

  const callModel = async (state: z.infer<typeof MessagesZodState>) => {
    const response = await model.invoke(state.messages);
    return { messages: [response] };
  };

  const builder = new StateGraph(MessagesZodState)
    .addNode("call_model", callModel)
    .addNode("delete_messages", deleteMessages)
    .addEdge(START, "call_model")
    .addEdge("call_model", "delete_messages");

  const checkpointer = new MemorySaver();
  const app = builder.compile({ checkpointer });

  const config = { configurable: { thread_id: "1" } };

  for await (const event of await app.stream(
    { messages: [{ role: "user", content: "hi! I'm bob" }] },
    { ...config, streamMode: "values" }
  )) {
    console.log(event.messages.map(message => [message.getType(), message.content]));
  }

  for await (const event of await app.stream(
    { messages: [{ role: "user", content: "what's my name?" }] },
    { ...config, streamMode: "values" }
  )) {
    console.log(event.messages.map(message => [message.getType(), message.content]));
  }
  ```

</Accordion>

#### Summarize messages

The problem with trimming or removing messages is that you may lose information. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.

![](/oss/images/summary.png)



Prompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can include a `summary` key in the state alongside the `messages` key:

```typescript
import { BaseMessage } from "@langchain/core/messages";
import { MessagesZodMeta } from "@langchain/langgraph";
import { registry } from "@langchain/langgraph/zod";
import * as z from "zod";

const State = z.object({
  messages: z
    .array(z.custom<BaseMessage>())
    .register(registry, MessagesZodMeta),
  summary: z.string().optional(),
});
```

Then, you can generate a summary of the chat history, using any existing summary as context for the next summary. This `summarizeConversation` node can be called after some number of messages have accumulated in the `messages` state key.

```typescript
import { RemoveMessage, HumanMessage } from "@langchain/core/messages";

const summarizeConversation = async (state: z.infer<typeof State>) => {
  // First, we get any existing summary
  const summary = state.summary || "";

  // Create our summarization prompt
  let summaryMessage: string;
  if (summary) {
    // A summary already exists
    summaryMessage =
      `This is a summary of the conversation to date: ${summary}\n\n` +
      "Extend the summary by taking into account the new messages above:";
  } else {
    summaryMessage = "Create a summary of the conversation above:";
  }

  // Add prompt to our history
  const messages = [
    ...state.messages,
    new HumanMessage({ content: summaryMessage })
  ];
  const response = await model.invoke(messages);

  // Delete all but the 2 most recent messages
  const deleteMessages = state.messages
    .slice(0, -2)
    .map(m => new RemoveMessage({ id: m.id }));

  return {
    summary: response.content,
    messages: deleteMessages
  };
};
```


<Accordion title="Full example: summarize messages">


  ```typescript
  import { ChatAnthropic } from "@langchain/anthropic";
  import {
    SystemMessage,
    HumanMessage,
    RemoveMessage,
    type BaseMessage
  } from "@langchain/core/messages";
  import {
    MessagesZodMeta,
    StateGraph,
    START,
    END,
    MemorySaver,
  } from "@langchain/langgraph";
  import { registry } from "@langchain/langgraph/zod";
  import * as z from "zod";
  import { v4 as uuidv4 } from "uuid";

  const memory = new MemorySaver();

  const GraphState = z.object({
    messages: z
      .array(z.custom<BaseMessage>())
      .register(registry, MessagesZodMeta),
    summary: z.string().default(""),
  });

  const model = new ChatAnthropic({ model: "claude-haiku-4-5-20251001" });

  const callModel = async (state: z.infer<typeof GraphState>) => {
    const { summary } = state;
    let { messages } = state;
    if (summary) {
      const systemMessage = new SystemMessage({
        id: uuidv4(),
        content: `Summary of conversation earlier: ${summary}`,
      });
      messages = [systemMessage, ...messages];
    }
    const response = await model.invoke(messages);
    return { messages: [response] };
  };

  const shouldContinue = (state: z.infer<typeof GraphState>) => {
    const messages = state.messages;
    if (messages.length > 6) {
      return "summarize_conversation";
    }
    return END;
  };

  const summarizeConversation = async (state: z.infer<typeof GraphState>) => {
    const { summary, messages } = state;
    let summaryMessage: string;
    if (summary) {
      summaryMessage =
        `This is summary of the conversation to date: ${summary}\n\n` +
        "Extend the summary by taking into account the new messages above:";
    } else {
      summaryMessage = "Create a summary of the conversation above:";
    }

    const allMessages = [
      ...messages,
      new HumanMessage({ id: uuidv4(), content: summaryMessage }),
    ];

    const response = await model.invoke(allMessages);

    const deleteMessages = messages
      .slice(0, -2)
      .map((m) => new RemoveMessage({ id: m.id! }));

    if (typeof response.content !== "string") {
      throw new Error("Expected a string response from the model");
    }

    return { summary: response.content, messages: deleteMessages };
  };

  const workflow = new StateGraph(GraphState)
    .addNode("conversation", callModel)
    .addNode("summarize_conversation", summarizeConversation)
    .addEdge(START, "conversation")
    .addConditionalEdges("conversation", shouldContinue)
    .addEdge("summarize_conversation", END);

  const app = workflow.compile({ checkpointer: memory });
  ```

</Accordion>

#### Manage checkpoints

You can view and delete the information stored by the checkpointer.

<Accordion title="View thread state">


  ```typescript
  const config = {
    configurable: {
      thread_id: "1",
      // optionally provide an ID for a specific checkpoint,
      // otherwise the latest checkpoint is shown
      // checkpoint_id: "1f029ca3-1f5b-6704-8004-820c16b69a5a"
    },
  };
  await graph.getState(config);
  ```

  ```
  {
    values: { messages: [HumanMessage(...), AIMessage(...), HumanMessage(...), AIMessage(...)] },
    next: [],
    config: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1f5b-6704-8004-820c16b69a5a' } },
    metadata: {
      source: 'loop',
      writes: { call_model: { messages: AIMessage(...) } },
      step: 4,
      parents: {},
      thread_id: '1'
    },
    createdAt: '2025-05-05T16:01:24.680462+00:00',
    parentConfig: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1790-6b0a-8003-baf965b6a38f' } },
    tasks: [],
    interrupts: []
  }
  ```

</Accordion>

<Accordion title="View the history of the thread">


  ```typescript
  const config = {
    configurable: {
      thread_id: "1",
    },
  };

  const history = [];
  for await (const state of graph.getStateHistory(config)) {
    history.push(state);
  }
  ```

</Accordion>

<Accordion title="Delete all checkpoints for a thread">


  ```typescript
  const threadId = "1";
  await checkpointer.deleteThread(threadId);
  ```

</Accordion>

### Checkpoint data

Checkpointers need to serialize state when saving it to storage. LangGraph provides flexible serialization options and supports encryption for sensitive data.

#### Serialization



Checkpointers include default serialization that handles:
- LangChain and LangGraph primitives
- JavaScript Date objects
- Common JavaScript types


#### Encryption



---

## Stores

Stores enable long-term, cross-thread memory for storing user-specific or application-level data that should persist across multiple conversations or workflow sessions.

![Model of shared state](/oss/images/shared_state.png)

While [checkpointers](#checkpointers) save state to a specific thread, stores allow you to share information **across threads**. For example, you might want to remember a user's preferences or facts about them across all of their conversations with your agent.

### Add stores



```typescript
import { InMemoryStore, StateGraph } from "@langchain/langgraph";

const store = new InMemoryStore();

const builder = new StateGraph(...);
const graph = builder.compile({ store });
```


Stores organize data using **namespaces** - tuples that help you categorize and retrieve information:



```typescript
// Store user preferences
const userId = "123";
let namespace = [userId, "preferences"];
await store.put(namespace, "theme", { value: "dark" });

// Store user facts
namespace = [userId, "facts"];
await store.put(namespace, "fact_1", { text: "Likes pizza" });
```


#### Use in production

In production, use a store backed by a database:



```typescript
import { PostgresStore } from "@langchain/langgraph-checkpoint-postgres/store";

const DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable";
const store = PostgresStore.fromConnString(DB_URI);

const builder = new StateGraph(...);
const graph = builder.compile({ store });
```


<Accordion title="Example: using Postgres store">


  ```
  npm install @langchain/langgraph-checkpoint-postgres
  ```

    <Tip>
    You need to call `store.setup()` the first time you're using Postgres store
    </Tip>

  ```typescript
  import { ChatAnthropic } from "@langchain/anthropic";
  import { StateGraph, MessagesZodMeta, START, LangGraphRunnableConfig } from "@langchain/langgraph";
  import { PostgresSaver } from "@langchain/langgraph-checkpoint-postgres";
  import { PostgresStore } from "@langchain/langgraph-checkpoint-postgres/store";
  import { BaseMessage } from "@langchain/core/messages";
  import { registry } from "@langchain/langgraph/zod";
  import * as z from "zod";
  import { v4 as uuidv4 } from "uuid";

  const MessagesZodState = z.object({
    messages: z
      .array(z.custom<BaseMessage>())
      .register(registry, MessagesZodMeta),
  });

  const model = new ChatAnthropic({ model: "claude-haiku-4-5-20251001" });

  const DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable";

  const store = PostgresStore.fromConnString(DB_URI);
  const checkpointer = PostgresSaver.fromConnString(DB_URI);
  // await store.setup();
  // await checkpointer.setup();

  const callModel = async (
    state: z.infer<typeof MessagesZodState>,
    config: LangGraphRunnableConfig,
  ) => {
    const userId = config.configurable?.userId;
    const namespace = ["memories", userId];
    const memories = await config.store?.search(namespace, { query: state.messages.at(-1)?.content });
    const info = memories?.map(d => d.value.data).join("\n") || "";
    const systemMsg = `You are a helpful assistant talking to the user. User info: ${info}`;

    // Store new memories if the user asks the model to remember
    const lastMessage = state.messages.at(-1);
    if (lastMessage?.content?.toLowerCase().includes("remember")) {
      const memory = "User name is Bob";
      await config.store?.put(namespace, uuidv4(), { data: memory });
    }

    const response = await model.invoke([
      { role: "system", content: systemMsg },
      ...state.messages
    ]);
    return { messages: [response] };
  };

  const builder = new StateGraph(MessagesZodState)
    .addNode("call_model", callModel)
    .addEdge(START, "call_model");

  const graph = builder.compile({
    checkpointer,
    store,
  });

  const config = {
    configurable: {
      thread_id: "1",
      userId: "1",
    }
  };

  for await (const chunk of await graph.stream(
    { messages: [{ role: "user", content: "Hi! Remember: my name is Bob" }] },
    { ...config, streamMode: "values" }
  )) {
    console.log(chunk.messages.at(-1)?.content);
  }

  const config2 = {
    configurable: {
      thread_id: "2",
      userId: "1",
    }
  };

  for await (const chunk of await graph.stream(
    { messages: [{ role: "user", content: "what is my name?" }] },
    { ...config2, streamMode: "values" }
  )) {
    console.log(chunk.messages.at(-1)?.content);
  }
  ```

</Accordion>



  ```
  npm install @langchain/langgraph-checkpoint-postgres
  ```

    <Tip>
    You need to call `store.setup()` the first time you're using Postgres store
    </Tip>

  ```typescript
  import { ChatAnthropic } from "@langchain/anthropic";
  import { StateGraph, MessagesZodMeta, START, LangGraphRunnableConfig } from "@langchain/langgraph";
  import { PostgresSaver } from "@langchain/langgraph-checkpoint-postgres";
  import { PostgresStore } from "@langchain/langgraph-checkpoint-postgres/store";
  import { BaseMessage } from "@langchain/core/messages";
  import { registry } from "@langchain/langgraph/zod";
  import * as z from "zod";
  import { v4 as uuidv4 } from "uuid";

  const MessagesZodState = z.object({
    messages: z
      .array(z.custom<BaseMessage>())
      .register(registry, MessagesZodMeta),
  });

  const model = new ChatAnthropic({ model: "claude-haiku-4-5-20251001" });

  const DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable";

  const store = PostgresStore.fromConnString(DB_URI);
  const checkpointer = PostgresSaver.fromConnString(DB_URI);
  // await store.setup();
  // await checkpointer.setup();

  const callModel = async (
    state: z.infer<typeof MessagesZodState>,
    config: LangGraphRunnableConfig,
  ) => {
    const userId = config.configurable?.userId;
    const namespace = ["memories", userId];
    const memories = await config.store?.search(namespace, { query: state.messages.at(-1)?.content });
    const info = memories?.map(d => d.value.data).join("\n") || "";
    const systemMsg = `You are a helpful assistant talking to the user. User info: ${info}`;

    // Store new memories if the user asks the model to remember
    const lastMessage = state.messages.at(-1);
    if (lastMessage?.content?.toLowerCase().includes("remember")) {
      const memory = "User name is Bob";
      await config.store?.put(namespace, uuidv4(), { data: memory });
    }

    const response = await model.invoke([
      { role: "system", content: systemMsg },
      ...state.messages
    ]);
    return { messages: [response] };
  };

  const builder = new StateGraph(MessagesZodState)
    .addNode("call_model", callModel)
    .addEdge(START, "call_model");

  const graph = builder.compile({
    checkpointer,
    store,
  });

  const config = {
    configurable: {
      thread_id: "1",
      userId: "1",
    }
  };

  for await (const chunk of await graph.stream(
    { messages: [{ role: "user", content: "Hi! Remember: my name is Bob" }] },
    { ...config, streamMode: "values" }
  )) {
    console.log(chunk.messages.at(-1)?.content);
  }

  const config2 = {
    configurable: {
      thread_id: "2",
      userId: "1",
    }
  };

  for await (const chunk of await graph.stream(
    { messages: [{ role: "user", content: "what is my name?" }] },
    { ...config2, streamMode: "values" }
  )) {
    console.log(chunk.messages.at(-1)?.content);
  }
  ```

</Accordion>

### Use semantic search

Enable semantic search in your graph's memory store to let graph agents search for items by semantic similarity instead of exact matches.



```typescript
import { OpenAIEmbeddings } from "@langchain/openai";
import { InMemoryStore } from "@langchain/langgraph";

// Create store with semantic search enabled
const embeddings = new OpenAIEmbeddings({ model: "text-embedding-3-small" });
const store = new InMemoryStore({
  index: {
    embeddings,
    dims: 1536,
  },
});

await store.put(["user_123", "memories"], "1", { text: "I love pizza" });
await store.put(["user_123", "memories"], "2", { text: "I am a plumber" });

const items = await store.search(["user_123", "memories"], {
  query: "I'm hungry",
  limit: 1,
});
```


<Accordion title="Full example: long-term memory with semantic search">


    ```typescript
    import { OpenAIEmbeddings, ChatOpenAI } from "@langchain/openai";
    import { StateGraph, START, MessagesZodMeta, InMemoryStore } from "@langchain/langgraph";
    import { BaseMessage } from "@langchain/core/messages";
    import { registry } from "@langchain/langgraph/zod";
    import * as z from "zod";

    const MessagesZodState = z.object({
        messages: z
        .array(z.custom<BaseMessage>())
        .register(registry, MessagesZodMeta),
    });

    const model = new ChatOpenAI({ model: "gpt-4o-mini" });

    // Create store with semantic search enabled
    const embeddings = new OpenAIEmbeddings({ model: "text-embedding-3-small" });
    const store = new InMemoryStore({
        index: {
        embeddings,
        dims: 1536,
        }
    });

    await store.put(["user_123", "memories"], "1", { text: "I love pizza" });
    await store.put(["user_123", "memories"], "2", { text: "I am a plumber" });

    const chat = async (state: z.infer<typeof MessagesZodState>, config) => {
        // Search based on user's last message
        const items = await config.store.search(
        ["user_123", "memories"],
        { query: state.messages.at(-1)?.content, limit: 2 }
        );
        const memories = items.map(item => item.value.text).join("\n");
        const memoriesText = memories ? `## Memories of user\n${memories}` : "";

        const response = await model.invoke([
        { role: "system", content: `You are a helpful assistant.\n${memoriesText}` },
        ...state.messages,
        ]);

        return { messages: [response] };
    };

    const builder = new StateGraph(MessagesZodState)
        .addNode("chat", chat)
        .addEdge(START, "chat");
    const graph = builder.compile({ store });

    for await (const [message, metadata] of await graph.stream(
        { messages: [{ role: "user", content: "I'm hungry" }] },
        { streamMode: "messages" }
    )) {
        if (message.content) {
        console.log(message.content);
        }
    }
    ```

</Accordion>

---

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/add-memory.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>
