---
title: Memory
---

AI applications need [memory](/oss/python/concepts/memory) to share context across multiple interactions. In LangGraph, you can add two types of memory:

* [**Checkpointers**](#checkpointers) - Thread-scoped persistence for multi-turn conversations and workflow state.
* [**Stores**](#stores) - Cross-thread persistence for user-specific or application-level data.

---

## Checkpointers

Checkpointers provide thread-level [persistence](/oss/python/langgraph/persistence), enabling your graph to:
- Track multi-turn conversations
- Resume after interruptions or failures ([durable execution](/oss/python/langgraph/durable-execution))
- Access historical states for debugging and [time travel](/oss/python/langgraph/use-time-travel)
- Enable [human-in-the-loop](/oss/python/langgraph/interrupts) workflows

### Add checkpointers

To add checkpointers to your graph:

```python
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import StateGraph

checkpointer = InMemorySaver()

builder = StateGraph(...)
graph = builder.compile(checkpointer=checkpointer)

graph.invoke(
    {"messages": [{"role": "user", "content": "hi! i am Bob"}]},
    {"configurable": {"thread_id": "1"}},
)
```




<Info>
**What is a thread?**

A thread is a unique conversation or workflow session identified by a `thread_id`. When you invoke your graph with a specific `thread_id`, LangGraph saves checkpoints (snapshots of the graph state) to that thread. All subsequent invocations with the same `thread_id` can access and continue from the saved state.
</Info>

#### Use in production

In production, use a checkpointer backed by a database:

```python
from langgraph.checkpoint.postgres import PostgresSaver

DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable"
with PostgresSaver.from_conn_string(DB_URI) as checkpointer:
    builder = StateGraph(...)
    graph = builder.compile(checkpointer=checkpointer)
```




<Accordion title="Example: using Postgres checkpointer">
  ```
  pip install -U "psycopg[binary,pool]" langgraph langgraph-checkpoint-postgres
  ```

    <Tip>
    You need to call `checkpointer.setup()` the first time you're using Postgres checkpointer
    </Tip>

    <Tabs>
        <Tab title="Sync">
      ```python
      from langchain.chat_models import init_chat_model
      from langgraph.graph import StateGraph, MessagesState, START
      from langgraph.checkpoint.postgres import PostgresSaver

      model = init_chat_model(model="claude-haiku-4-5-20251001")

      DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable"
      with PostgresSaver.from_conn_string(DB_URI) as checkpointer:
          # checkpointer.setup()

          def call_model(state: MessagesState):
              response = model.invoke(state["messages"])
              return {"messages": response}

          builder = StateGraph(MessagesState)
          builder.add_node(call_model)
          builder.add_edge(START, "call_model")

          graph = builder.compile(checkpointer=checkpointer)

          config = {
              "configurable": {
                  "thread_id": "1"
              }
          }

          for chunk in graph.stream(
              {"messages": [{"role": "user", "content": "hi! I'm bob"}]},
              config,
              stream_mode="values"
          ):
              chunk["messages"][-1].pretty_print()

          for chunk in graph.stream(
              {"messages": [{"role": "user", "content": "what's my name?"}]},
              config,
              stream_mode="values"
          ):
              chunk["messages"][-1].pretty_print()
```
        </Tab>
        <Tab title="Async">
      ```python
      from langchain.chat_models import init_chat_model
      from langgraph.graph import StateGraph, MessagesState, START
      from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver

      model = init_chat_model(model="claude-haiku-4-5-20251001")

      DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable"
      async with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:
          # await checkpointer.setup()

          async def call_model(state: MessagesState):
              response = await model.ainvoke(state["messages"])
              return {"messages": response}

          builder = StateGraph(MessagesState)
          builder.add_node(call_model)
          builder.add_edge(START, "call_model")

          graph = builder.compile(checkpointer=checkpointer)

          config = {
              "configurable": {
                  "thread_id": "1"
              }
          }

          async for chunk in graph.astream(
              {"messages": [{"role": "user", "content": "hi! I'm bob"}]},
              config,
              stream_mode="values"
          ):
              chunk["messages"][-1].pretty_print()

          async for chunk in graph.astream(
              {"messages": [{"role": "user", "content": "what's my name?"}]},
              config,
              stream_mode="values"
          ):
              chunk["messages"][-1].pretty_print()
```
        </Tab>
    </Tabs>



</Accordion>

<Accordion title="Example: using MongoDB checkpointer">
  ```
  pip install -U pymongo langgraph langgraph-checkpoint-mongodb
  ```

    <Note>
    **Setup**
    To use the MongoDB checkpointer, you will need a MongoDB cluster. Follow [this guide](https://www.mongodb.com/docs/guides/atlas/cluster/) to create a cluster if you don't already have one.
    </Note>

    <Tabs>
        <Tab title="Sync">
      ```python
      from langchain.chat_models import init_chat_model
      from langgraph.graph import StateGraph, MessagesState, START
      from langgraph.checkpoint.mongodb import MongoDBSaver

      model = init_chat_model(model="claude-haiku-4-5-20251001")

      DB_URI = "localhost:27017"
      with MongoDBSaver.from_conn_string(DB_URI) as checkpointer:

          def call_model(state: MessagesState):
              response = model.invoke(state["messages"])
              return {"messages": response}

          builder = StateGraph(MessagesState)
          builder.add_node(call_model)
          builder.add_edge(START, "call_model")

          graph = builder.compile(checkpointer=checkpointer)

          config = {
              "configurable": {
                  "thread_id": "1"
              }
          }

          for chunk in graph.stream(
              {"messages": [{"role": "user", "content": "hi! I'm bob"}]},
              config,
              stream_mode="values"
          ):
              chunk["messages"][-1].pretty_print()

          for chunk in graph.stream(
              {"messages": [{"role": "user", "content": "what's my name?"}]},
              config,
              stream_mode="values"
          ):
              chunk["messages"][-1].pretty_print()
```
        </Tab>
        <Tab title="Async">
      ```python
      from langchain.chat_models import init_chat_model
      from langgraph.graph import StateGraph, MessagesState, START
      from langgraph.checkpoint.mongodb.aio import AsyncMongoDBSaver

      model = init_chat_model(model="claude-haiku-4-5-20251001")

      DB_URI = "localhost:27017"
      async with AsyncMongoDBSaver.from_conn_string(DB_URI) as checkpointer:

          async def call_model(state: MessagesState):
              response = await model.ainvoke(state["messages"])
              return {"messages": response}

          builder = StateGraph(MessagesState)
          builder.add_node(call_model)
          builder.add_edge(START, "call_model")

          graph = builder.compile(checkpointer=checkpointer)

          config = {
              "configurable": {
                  "thread_id": "1"
              }
          }

          async for chunk in graph.astream(
              {"messages": [{"role": "user", "content": "hi! I'm bob"}]},
              config,
              stream_mode="values"
          ):
              chunk["messages"][-1].pretty_print()

          async for chunk in graph.astream(
              {"messages": [{"role": "user", "content": "what's my name?"}]},
              config,
              stream_mode="values"
          ):
              chunk["messages"][-1].pretty_print()
```
        </Tab>
    </Tabs>
</Accordion>

<Accordion title="Example: using Redis checkpointer">
  ```
  pip install -U langgraph langgraph-checkpoint-redis
  ```

    <Tip>
    You need to call `checkpointer.setup()` the first time you're using Redis checkpointer
    </Tip>

    <Tabs>
        <Tab title="Sync">
      ```python
      from langchain.chat_models import init_chat_model
      from langgraph.graph import StateGraph, MessagesState, START
      from langgraph.checkpoint.redis import RedisSaver

      model = init_chat_model(model="claude-haiku-4-5-20251001")

      DB_URI = "redis://localhost:6379"
      with RedisSaver.from_conn_string(DB_URI) as checkpointer:
          # checkpointer.setup()

          def call_model(state: MessagesState):
              response = model.invoke(state["messages"])
              return {"messages": response}

          builder = StateGraph(MessagesState)
          builder.add_node(call_model)
          builder.add_edge(START, "call_model")

          graph = builder.compile(checkpointer=checkpointer)

          config = {
              "configurable": {
                  "thread_id": "1"
              }
          }

          for chunk in graph.stream(
              {"messages": [{"role": "user", "content": "hi! I'm bob"}]},
              config,
              stream_mode="values"
          ):
              chunk["messages"][-1].pretty_print()

          for chunk in graph.stream(
              {"messages": [{"role": "user", "content": "what's my name?"}]},
              config,
              stream_mode="values"
          ):
              chunk["messages"][-1].pretty_print()
```
        </Tab>
        <Tab title="Async">
      ```python
      from langchain.chat_models import init_chat_model
      from langgraph.graph import StateGraph, MessagesState, START
      from langgraph.checkpoint.redis.aio import AsyncRedisSaver

      model = init_chat_model(model="claude-haiku-4-5-20251001")

      DB_URI = "redis://localhost:6379"
      async with AsyncRedisSaver.from_conn_string(DB_URI) as checkpointer:
          # await checkpointer.asetup()

          async def call_model(state: MessagesState):
              response = await model.ainvoke(state["messages"])
              return {"messages": response}

          builder = StateGraph(MessagesState)
          builder.add_node(call_model)
          builder.add_edge(START, "call_model")

          graph = builder.compile(checkpointer=checkpointer)

          config = {
              "configurable": {
                  "thread_id": "1"
              }
          }

          async for chunk in graph.astream(
              {"messages": [{"role": "user", "content": "hi! I'm bob"}]},
              config,
              stream_mode="values"
          ):
              chunk["messages"][-1].pretty_print()

          async for chunk in graph.astream(
              {"messages": [{"role": "user", "content": "what's my name?"}]},
              config,
              stream_mode="values"
          ):
              chunk["messages"][-1].pretty_print()
```
        </Tab>
    </Tabs>
</Accordion>


### Checkpointers in subgraphs

When your graph contains [subgraphs](/oss/python/langgraph/use-subgraphs), understanding how checkpointing works is critical for building multi-agent systems and complex workflows. The behavior depends on which checkpointer option you use when compiling the subgraph.

<Info>
**LangGraph API handles subgraph checkpointing automatically**

When using the LangGraph API, checkpointing is configured automatically for subgraphs. The information below is relevant when self-hosting or when you need to understand the behavior.
</Info>

#### Checkpointer options

When compiling a subgraph, you can control how its state is persisted:

```python
# Option 1: No persistent state across invocations (default)
subgraph = subgraph_builder.compile()

# Option 2: Persistent state across invocations
subgraph = subgraph_builder.compile(checkpointer=True)

# Option 3: No checkpointing at all
subgraph = subgraph_builder.compile(checkpointer=False)

# Alternative: Use a separate checkpointer instance
from langgraph.checkpoint.memory import InMemorySaver
subgraph = subgraph_builder.compile(checkpointer=InMemorySaver())
```




The most common options are `checkpointer=None` (default) and `checkpointer=True`, which both use the parent's checkpointer but differ in how they namespace the subgraph's state. You can also pass any `BaseCheckpointSaver` instance to give the subgraph its own separate checkpointer, though the common practice for leveraging functionality like `interrupt()` in subgraphs is to use the parent's checkpointer with either `checkpointer=True` or the default `checkpointer=None`.

<Tabs>
  <Tab title="checkpointer=None (default)">
    With the default option, the subgraph uses a dynamic namespace that changes with each invocation. This means the subgraph state resets between runs - each invocation starts with a clean slate. The state from each run is still stored and accessible by the parent graph, but the subgraph itself doesn't carry over state from previous invocations.

    Use this when your subgraph acts as a reusable tool that may be called multiple times and each invocation should start fresh, such as a search tool or data processing pipeline.

    ```python expandable
    from langgraph.graph import StateGraph
    from langgraph.checkpoint.memory import InMemorySaver
    from typing_extensions import TypedDict
    from typing import Annotated
    import operator

    checkpointer = InMemorySaver()

    class State(TypedDict):
        parent: Annotated[int, operator.add]

    class SubgraphState(TypedDict):
        subgraph: Annotated[int, operator.add]

    def node_a(state: State):
        return {"parent": 1}

    def node_b(state: SubgraphState):
        return {"subgraph": 1}

    # Subgraph with checkpointer=None (default)
    subgraph_builder = StateGraph(SubgraphState)
    subgraph_builder.add_node("node_b", node_b)
    subgraph_builder.set_entry_point("node_b")
    subgraph = subgraph_builder.compile()  # checkpointer=None

    # Parent graph
    builder = StateGraph(State)
    builder.add_node("node_a", node_a)
    builder.add_node("subgraph", subgraph)
    builder.set_entry_point("node_a")
    builder.add_edge("node_a", "subgraph")
    graph = builder.compile(checkpointer=checkpointer)

    config = {"configurable": {"thread_id": "1"}}

    # Run 1
    for chunk in graph.stream({}, subgraphs=True, stream_mode="values", config=config):
        print(chunk)
    # Output:
    # ((), {'parent': 1})
    # (('subgraph:ac5cc169-30a2-7d6e-ae2a-00800959aeb8',), {'subgraph': 1})

    # Run 2
    for chunk in graph.stream({}, subgraphs=True, stream_mode="values", config=config):
        print(chunk)
    # Output:
    # ((), {'parent': 2})  # parent accumulated
    # (('subgraph:b84c6daf-b464-1756-8b39-30d38e101fcf',), {'subgraph': 1})  # subgraph reset
    ```

    Notice the subgraph namespace changes each run (`ac5cc169...` vs `b84c6daf...`), and `subgraph` value resets to `1` each time.


  </Tab>

  <Tab title="checkpointer=True">
    When you use `checkpointer=True`, the subgraph uses a static namespace that stays the same across invocations. This means the subgraph state persists across runs within the same thread. The subgraph resumes from its last execution, replaying from the last stored checkpoint. Note that this is not time-linear - it replays from the last checkpoint in the namespace, regardless of when that execution occurred.

    Use this for multi-agent systems where each agent should maintain its own conversation history, or when building workflows where subgraphs need to remember things across multiple invocations.

    ```python expandable
    from langgraph.graph import StateGraph
    from langgraph.checkpoint.memory import InMemorySaver
    from typing_extensions import TypedDict
    from typing import Annotated
    import operator

    checkpointer = InMemorySaver()

    class State(TypedDict):
        parent: Annotated[int, operator.add]

    class SubgraphState(TypedDict):
        subgraph: Annotated[int, operator.add]

    def node_a(state: State):
        return {"parent": 1}

    def node_b(state: SubgraphState):
        return {"subgraph": 1}

    # Subgraph with checkpointer=True
    subgraph_builder = StateGraph(SubgraphState)
    subgraph_builder.add_node("node_b", node_b)
    subgraph_builder.set_entry_point("node_b")
    subgraph = subgraph_builder.compile(checkpointer=True)  # State persists!

    # Parent graph
    builder = StateGraph(State)
    builder.add_node("node_a", node_a)
    builder.add_node("subgraph", subgraph)
    builder.set_entry_point("node_a")
    builder.add_edge("node_a", "subgraph")
    graph = builder.compile(checkpointer=checkpointer)

    config = {"configurable": {"thread_id": "1"}}

    # Run 1
    for chunk in graph.stream({}, subgraphs=True, stream_mode="values", config=config):
        print(chunk)
    # Output:
    # ((), {'parent': 1})
    # (('subgraph',), {'subgraph': 1})

    # Run 2
    for chunk in graph.stream({}, subgraphs=True, stream_mode="values", config=config):
        print(chunk)
    # Output:
    # ((), {'parent': 2})  # parent accumulated
    # (('subgraph',), {'subgraph': 2})  # subgraph accumulated!
    ```

    Notice the subgraph namespace stays stable (`'subgraph'`), and the `subgraph` value accumulates across runs (`1` â†’ `2`).


  </Tab>

  <Tab title="checkpointer=False">
    When you use `checkpointer=False`, the subgraph does not use checkpointing at all. No state is saved anywhere, even if the parent graph has a checkpointer configured.

    Use this when the subgraph is purely stateless and you want to avoid any persistence overhead.
  </Tab>
</Tabs>

<Tip>
If your subgraph is used like a function that might be called multiple times (e.g., a tool in a ReAct agent), use `checkpointer=None`. If your subgraph represents an entity with its own memory (e.g., an agent with conversation history), use `checkpointer=True`.
</Tip>

### Access state

With checkpointing enabled, you can access the current state, historical states, and subgraph states.

#### Get current state

You can retrieve the current state of your graph at any time, including the state values and the next nodes that will execute.

```python
config = {"configurable": {"thread_id": "1"}}
state = graph.get_state(config)

print(state.values)  # Current state values
print(state.next)    # Next nodes to execute
```




#### Get state history

You can access the full execution history for a thread to see how the state evolved over time. The history is ordered chronologically from most recent to oldest.

```python
config = {"configurable": {"thread_id": "1"}}
history = list(graph.get_state_history(config))

# History is ordered from most recent to oldest
for state in history:
    print(f"Step: {state.metadata['step']}, Values: {state.values}")
```




<Accordion title="View full state snapshot structure">
  ```python
  StateSnapshot(
      values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob!')]},
      next=(),
      config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},
      metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob!')}}, 'step': 2},
      created_at='2024-08-29T19:19:38.821749+00:00',
      parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},
      tasks=()
  )
  ```



</Accordion>

#### Get subgraph state

To access subgraph state, use `subgraphs=True`:

```python
config = {"configurable": {"thread_id": "1"}}
state = graph.get_state(config, subgraphs=True)

# Access subgraph state from tasks
for task in state.tasks:
    if task.state:
        print(f"Subgraph namespace: {task.state.config['configurable']['checkpoint_ns']}")
        print(f"Subgraph values: {task.state.values}")
```




#### Get subgraph state history

To get a subgraph's state history, first get the subgraph's `checkpoint_ns` from `get_state(subgraphs=True)`, then call `get_state_history` with that namespace:

```python
# Step 1: Get the subgraph's checkpoint namespace
config = {"configurable": {"thread_id": "1"}}
state = graph.get_state(config, subgraphs=True)
subgraph_config = state.tasks[0].state.config
subgraph_ns = subgraph_config["configurable"]["checkpoint_ns"]

# Step 2: Get subgraph history using that namespace
subgraph_history = list(graph.get_state_history({
    "configurable": {
        "thread_id": "1",
        "checkpoint_ns": subgraph_ns
    }
}))

for h in subgraph_history:
    print(f"Subgraph step: {h.metadata['step']}, Values: {h.values}")
```

<Info>
**Why doesn't `get_state_history()` have a `subgraphs=True` option?**

`get_state_history()` returns an Iterator to avoid loading all checkpoints into memory at once. Adding `subgraphs=True` would require loading all checkpoints from all nested subgraphs simultaneously, which could cause huge memory overhead. Instead, you explicitly get each subgraph's namespace and fetch its history separately.
</Info>




<Accordion title="Full example: accessing subgraph state and history">
  ```python
  from langgraph.graph import StateGraph
  from langgraph.checkpoint.memory import InMemorySaver
  from typing_extensions import TypedDict
  from langgraph.types import interrupt

  checkpointer = InMemorySaver()

  class State(TypedDict):
      step: str

  def node_a(state: State):
      return {"step": "a"}

  def node_b(state: State):
      return {"step": "b"}

  def node_c(state: State):
      return {"step": "c"}

  def node_d(state: State):
      interrupt("")
      return {"step": "d"}

  # Build subgraph
  subgraph_builder = StateGraph(State)
  subgraph_builder.add_node("node_b", node_b)
  subgraph_builder.add_node("node_c", node_c)
  subgraph_builder.add_node("node_d", node_d)
  subgraph_builder.set_entry_point("node_b")
  subgraph_builder.add_edge("node_b", "node_c")
  subgraph_builder.add_edge("node_c", "node_d")
  subgraph = subgraph_builder.compile()  # checkpointer=None

  # Build parent graph
  builder = StateGraph(State)
  builder.add_node("node_a", node_a)
  builder.add_node("subgraph", subgraph)
  builder.set_entry_point("node_a")
  builder.add_edge("node_a", "subgraph")
  graph = builder.compile(checkpointer=checkpointer)

  # Run the graph
  config = {"configurable": {"thread_id": "1"}}
  graph.invoke({"step": "START"}, config)

  # Get subgraph state and history
  state = graph.get_state(config, subgraphs=True)
  subgraph_config = state.tasks[0].state.config
  subgraph_ns = subgraph_config["configurable"]["checkpoint_ns"]
  print(f"Subgraph namespace: {subgraph_ns}\n")

  state_history = list(graph.get_state_history({
      "configurable": {
          "checkpoint_ns": subgraph_ns,
          "thread_id": "1"
      }
  }))

  for h in state_history:
      print(f"Step: {h.metadata['step']}, Values: {h.values}")
  ```



</Accordion>

### Manage checkpointers

With checkpointing enabled, long conversations can exceed the LLM's context window. Common solutions are:

* [Trim messages](#trim-messages) - Remove first or last N messages (before calling LLM)
* [Delete messages](#delete-messages) - Remove messages from LangGraph state permanently
* [Summarize messages](#summarize-messages) - Summarize earlier messages and replace them with a summary
* [Manage checkpoints](#manage-checkpoints) - Store and retrieve message history
* Custom strategies (e.g., message filtering)

#### Trim messages

Most LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `max_tokens`) to use for handling the boundary.



To trim message history, use the [`trim_messages`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) function:

```python
from langchain_core.messages.utils import trim_messages, count_tokens_approximately

def call_model(state: MessagesState):
    messages = trim_messages(
        state["messages"],
        strategy="last",
        token_counter=count_tokens_approximately,
        max_tokens=128,
        start_on="human",
        end_on=("human", "tool"),
    )
    response = model.invoke(messages)
    return {"messages": [response]}

builder = StateGraph(MessagesState)
builder.add_node(call_model)
# ...
```




```python expandable
from langchain_core.messages.utils import trim_messages, count_tokens_approximately
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START, MessagesState
from langgraph.checkpoint.memory import InMemorySaver

model = init_chat_model("claude-sonnet-4-5-20250929")

def call_model(state: MessagesState):
    messages = trim_messages(
        state["messages"],
        strategy="last",
        token_counter=count_tokens_approximately,
        max_tokens=128,
        start_on="human",
        end_on=("human", "tool"),
    )
    response = model.invoke(messages)
    return {"messages": [response]}

checkpointer = InMemorySaver()
builder = StateGraph(MessagesState)
builder.add_node(call_model)
builder.add_edge(START, "call_model")
graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "1"}}
graph.invoke({"messages": "hi, my name is bob"}, config)
graph.invoke({"messages": "write a short poem about cats"}, config)
graph.invoke({"messages": "now do the same but for dogs"}, config)
final_response = graph.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
```

```
================================== Ai Message ==================================

Your name is Bob, as you mentioned when you first introduced yourself.
```




#### Delete messages

You can delete messages from the graph state to manage the message history.

There are two ways to delete messages from the graph state:

**Remove specific messages** using `RemoveMessage`:

```python
from langchain.messages import RemoveMessage

def delete_messages(state):
    messages = state["messages"]
    if len(messages) > 2:
        # Remove the earliest two messages
        return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}
```

**Remove all messages** using `Overwrite`:

```python
from langgraph.types import Overwrite

def delete_all_messages(state):
    # Bypass the add_messages reducer and clear all messages
    return {"messages": Overwrite([])}
```

<Note>
`RemoveMessage` works with the `add_messages` [reducer](/oss/python/langgraph/graph-api#reducers) (like in [`MessagesState`](/oss/python/langgraph/graph-api#messagesstate)), while `Overwrite` bypasses the reducer entirely. [Learn more about Overwrite](/oss/python/langgraph/use-graph-api#bypass-reducers-with-overwrite).
</Note>




<Warning>
When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:

* Some providers expect message history to start with a `user` message
* Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.
</Warning>

<Accordion title="Full example: delete messages">
  ```python
  from langchain.messages import RemoveMessage
  from langchain.chat_models import init_chat_model
  from langgraph.graph import StateGraph, START, MessagesState
  from langgraph.checkpoint.memory import InMemorySaver

  model = init_chat_model("claude-haiku-4-5-20251001")

  def delete_messages(state):
      messages = state["messages"]
      if len(messages) > 2:
          return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}

  def call_model(state: MessagesState):
      response = model.invoke(state["messages"])
      return {"messages": response}

  builder = StateGraph(MessagesState)
  builder.add_sequence([call_model, delete_messages])
  builder.add_edge(START, "call_model")

  checkpointer = InMemorySaver()
  app = builder.compile(checkpointer=checkpointer)

  config = {"configurable": {"thread_id": "1"}}

  for event in app.stream(
      {"messages": [{"role": "user", "content": "hi! I'm bob"}]},
      config,
      stream_mode="values"
  ):
      print([(message.type, message.content) for message in event["messages"]])

  for event in app.stream(
      {"messages": [{"role": "user", "content": "what's my name?"}]},
      config,
      stream_mode="values"
  ):
      print([(message.type, message.content) for message in event["messages"]])
  ```



</Accordion>

#### Summarize messages

The problem with trimming or removing messages is that you may lose information. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.

![](/oss/images/summary.png)

Prompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can extend the [`MessagesState`](/oss/python/langgraph/graph-api#working-with-messages-in-graph-state) to include a `summary` key:

```python
from langgraph.graph import MessagesState
class State(MessagesState):
    summary: str
```

Then, you can generate a summary of the chat history, using any existing summary as context for the next summary. This `summarize_conversation` node can be called after some number of messages have accumulated in the `messages` state key.

```python
def summarize_conversation(state: State):

    # First, we get any existing summary
    summary = state.get("summary", "")

    # Create our summarization prompt
    if summary:

        # A summary already exists
        summary_message = (
            f"This is a summary of the conversation to date: {summary}\n\n"
            "Extend the summary by taking into account the new messages above:"
        )

    else:
        summary_message = "Create a summary of the conversation above:"

    # Add prompt to our history
    messages = state["messages"] + [HumanMessage(content=summary_message)]
    response = model.invoke(messages)

    # Delete all but the 2 most recent messages
    delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]
    return {"summary": response.content, "messages": delete_messages}
```




<Accordion title="Full example: summarize messages">
  ```python
  from typing import Any, TypedDict

  from langchain.chat_models import init_chat_model
  from langchain.messages import AnyMessage
  from langchain_core.messages.utils import count_tokens_approximately
  from langgraph.graph import StateGraph, START, MessagesState
  from langgraph.checkpoint.memory import InMemorySaver
  from langmem.short_term import SummarizationNode, RunningSummary

  model = init_chat_model("claude-sonnet-4-5-20250929")
  summarization_model = model.bind(max_tokens=128)

  class State(MessagesState):
      context: dict[str, RunningSummary]

  class LLMInputState(TypedDict):
      summarized_messages: list[AnyMessage]
      context: dict[str, RunningSummary]

  summarization_node = SummarizationNode(
      token_counter=count_tokens_approximately,
      model=summarization_model,
      max_tokens=256,
      max_tokens_before_summary=256,
      max_summary_tokens=128,
  )

  def call_model(state: LLMInputState):
      response = model.invoke(state["summarized_messages"])
      return {"messages": [response]}

  checkpointer = InMemorySaver()
  builder = StateGraph(State)
  builder.add_node(call_model)
  builder.add_node("summarize", summarization_node)
  builder.add_edge(START, "summarize")
  builder.add_edge("summarize", "call_model")
  graph = builder.compile(checkpointer=checkpointer)

  # Invoke the graph
  config = {"configurable": {"thread_id": "1"}}
  graph.invoke({"messages": "hi, my name is bob"}, config)
  graph.invoke({"messages": "write a short poem about cats"}, config)
  graph.invoke({"messages": "now do the same but for dogs"}, config)
  final_response = graph.invoke({"messages": "what's my name?"}, config)

  final_response["messages"][-1].pretty_print()
  print("\nSummary:", final_response["context"]["running_summary"].summary)
  ```



</Accordion>

#### Manage checkpoints

You can view and delete the information stored by the checkpointer.

<Accordion title="View thread state">
  <Tabs>
      <Tab title="Graph/Functional API">
      ```python
      config = {
          "configurable": {
              "thread_id": "1",
              # optionally provide an ID for a specific checkpoint,
              # otherwise the latest checkpoint is shown
              # "checkpoint_id": "1f029ca3-1f5b-6704-8004-820c16b69a5a"

          }
      }
      graph.get_state(config)
  ```

      ```
      StateSnapshot(
          values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]}, next=(),
          config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
          metadata={
              'source': 'loop',
              'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},
              'step': 4,
              'parents': {},
              'thread_id': '1'
          },
          created_at='2025-05-05T16:01:24.680462+00:00',
          parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
          tasks=(),
          interrupts=()
      )
      ```
      </Tab>
      <Tab title="Checkpointer API">
      ```python
      config = {
          "configurable": {
              "thread_id": "1",
              # optionally provide an ID for a specific checkpoint,
              # otherwise the latest checkpoint is shown
              # "checkpoint_id": "1f029ca3-1f5b-6704-8004-820c16b69a5a"

          }
      }
      checkpointer.get_tuple(config)
  ```

      ```
      CheckpointTuple(
          config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
          checkpoint={
              'v': 3,
              'ts': '2025-05-05T16:01:24.680462+00:00',
              'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',
              'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'}, 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},
              'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},
          },
          metadata={
              'source': 'loop',
              'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},
              'step': 4,
              'parents': {},
              'thread_id': '1'
          },
          parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
          pending_writes=[]
      )
      ```
      </Tab>
  </Tabs>



</Accordion>

<Accordion title="View the history of the thread">
  <Tabs>
      <Tab title="Graph/Functional API">
      ```python
      config = {
          "configurable": {
              "thread_id": "1"
          }
      }
      list(graph.get_state_history(config))
  ```

      ```
      [
          StateSnapshot(...),
          StateSnapshot(...),
          StateSnapshot(...),
          # ... more checkpoints
      ]
      ```
      </Tab>
      <Tab title="Checkpointer API">
      ```python
      config = {
          "configurable": {
              "thread_id": "1"
          }
      }
      list(checkpointer.list(config))
  ```

      ```
      [
          CheckpointTuple(...),
          CheckpointTuple(...),
          CheckpointTuple(...),
          # ... more checkpoints
      ]
      ```
      </Tab>
  </Tabs>



</Accordion>

<Accordion title="Delete all checkpoints for a thread">
  ```python
  thread_id = "1"
  checkpointer.delete_thread(thread_id)
  ```



</Accordion>

### Checkpoint data

Checkpointers need to serialize state when saving it to storage. LangGraph provides flexible serialization options and supports encryption for sensitive data.

#### Serialization

Checkpointers use `JsonPlusSerializer` by default, which handles:
- LangChain and LangGraph primitives
- Python datetimes
- Enums
- Common Python types

For objects not supported by the default serializer (e.g., Pandas dataframes), use pickle fallback:

```python
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer

checkpointer = InMemorySaver(
    serde=JsonPlusSerializer(pickle_fallback=True)
)
```




#### Encryption

To encrypt all persisted state, pass an instance of [`EncryptedSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer) to the `serde` argument of any checkpointer. The easiest way is via [`from_pycryptodome_aes`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer.from_pycryptodome_aes), which reads the AES key from the `LANGGRAPH_AES_KEY` environment variable:

<Tabs>
  <Tab title="SQLite">
  ```python
  import sqlite3
  from langgraph.checkpoint.serde.encrypted import EncryptedSerializer
  from langgraph.checkpoint.sqlite import SqliteSaver

  serde = EncryptedSerializer.from_pycryptodome_aes()  # reads LANGGRAPH_AES_KEY
  checkpointer = SqliteSaver(sqlite3.connect("checkpoint.db"), serde=serde)
  ```
  </Tab>

  <Tab title="Postgres">
  ```python
  from langgraph.checkpoint.serde.encrypted import EncryptedSerializer
  from langgraph.checkpoint.postgres import PostgresSaver

  serde = EncryptedSerializer.from_pycryptodome_aes()  # reads LANGGRAPH_AES_KEY
  checkpointer = PostgresSaver.from_conn_string("postgresql://...", serde=serde)
  checkpointer.setup()
  ```
  </Tab>
</Tabs>

<Note>
When running on LangSmith, encryption is automatically enabled whenever `LANGGRAPH_AES_KEY` is present. Other encryption schemes can be used by implementing [`CipherProtocol`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.base.CipherProtocol) and supplying it to [`EncryptedSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer).
</Note>


---

## Stores

Stores enable long-term, cross-thread memory for storing user-specific or application-level data that should persist across multiple conversations or workflow sessions.

![Model of shared state](/oss/images/shared_state.png)

While [checkpointers](#checkpointers) save state to a specific thread, stores allow you to share information **across threads**. For example, you might want to remember a user's preferences or facts about them across all of their conversations with your agent.

### Add stores

```python
from langgraph.store.memory import InMemoryStore
from langgraph.graph import StateGraph

store = InMemoryStore()

builder = StateGraph(...)
graph = builder.compile(store=store)
```




Stores organize data using **namespaces** - tuples that help you categorize and retrieve information:

```python
# Store user preferences
user_id = "123"
namespace = (user_id, "preferences")
store.put(namespace, "theme", {"value": "dark"})

# Store user facts
namespace = (user_id, "facts")
store.put(namespace, "fact_1", {"text": "Likes pizza"})
```




#### Use in production

In production, use a store backed by a database:

```python
from langgraph.store.postgres import PostgresStore

DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable"
with PostgresStore.from_conn_string(DB_URI) as store:
    builder = StateGraph(...)
    graph = builder.compile(store=store)
```




<Accordion title="Example: using Postgres store">
  ```
  pip install -U "psycopg[binary,pool]" langgraph langgraph-checkpoint-postgres
  ```

    <Tip>
    You need to call `store.setup()` the first time you're using Postgres store
    </Tip>

    <Tabs>
        <Tab title="Sync">
      ```python
      from langchain_core.runnables import RunnableConfig
      from langchain.chat_models import init_chat_model
      from langgraph.graph import StateGraph, MessagesState, START
      from langgraph.checkpoint.postgres import PostgresSaver
      from langgraph.store.postgres import PostgresStore
      from langgraph.store.base import BaseStore
      import uuid

      model = init_chat_model(model="claude-haiku-4-5-20251001")

      DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable"

      with (
          PostgresStore.from_conn_string(DB_URI) as store,
          PostgresSaver.from_conn_string(DB_URI) as checkpointer,
      ):
          # store.setup()
          # checkpointer.setup()

          def call_model(
              state: MessagesState,
              config: RunnableConfig,
              *,
              store: BaseStore,
          ):
              user_id = config["configurable"]["user_id"]
              namespace = ("memories", user_id)
              memories = store.search(namespace, query=str(state["messages"][-1].content))
              info = "\n".join([d.value["data"] for d in memories])
              system_msg = f"You are a helpful assistant talking to the user. User info: {info}"

              # Store new memories if the user asks the model to remember
              last_message = state["messages"][-1]
              if "remember" in last_message.content.lower():
                  memory = "User name is Bob"
                  store.put(namespace, str(uuid.uuid4()), {"data": memory})

              response = model.invoke(
                  [{"role": "system", "content": system_msg}] + state["messages"]
              )
              return {"messages": response}

          builder = StateGraph(MessagesState)
          builder.add_node(call_model)
          builder.add_edge(START, "call_model")

          graph = builder.compile(
              checkpointer=checkpointer,
              store=store,
          )

          config = {
              "configurable": {
                  "thread_id": "1",
                  "user_id": "1",
              }
          }
          for chunk in graph.stream(
              {"messages": [{"role": "user", "content": "Hi! Remember: my name is Bob"}]},
              config,
              stream_mode="values",
          ):
              chunk["messages"][-1].pretty_print()

          config = {
              "configurable": {
                  "thread_id": "2",
                  "user_id": "1",
              }
          }

          for chunk in graph.stream(
              {"messages": [{"role": "user", "content": "what is my name?"}]},
              config,
              stream_mode="values",
          ):
              chunk["messages"][-1].pretty_print()
```
        </Tab>
        <Tab title="Async">
      ```python
      from langchain_core.runnables import RunnableConfig
      from langchain.chat_models import init_chat_model
      from langgraph.graph import StateGraph, MessagesState, START
      from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
      from langgraph.store.postgres.aio import AsyncPostgresStore
      from langgraph.store.base import BaseStore
      import uuid

      model = init_chat_model(model="claude-haiku-4-5-20251001")

      DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable"

      async with (
          AsyncPostgresStore.from_conn_string(DB_URI) as store,
          AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer,
      ):
          # await store.setup()
          # await checkpointer.setup()

          async def call_model(
              state: MessagesState,
              config: RunnableConfig,
              *,
              store: BaseStore,
          ):
              user_id = config["configurable"]["user_id"]
              namespace = ("memories", user_id)
              memories = await store.asearch(namespace, query=str(state["messages"][-1].content))
              info = "\n".join([d.value["data"] for d in memories])
              system_msg = f"You are a helpful assistant talking to the user. User info: {info}"

              # Store new memories if the user asks the model to remember
              last_message = state["messages"][-1]
              if "remember" in last_message.content.lower():
                  memory = "User name is Bob"
                  await store.aput(namespace, str(uuid.uuid4()), {"data": memory})

              response = await model.ainvoke(
                  [{"role": "system", "content": system_msg}] + state["messages"]
              )
              return {"messages": response}

          builder = StateGraph(MessagesState)
          builder.add_node(call_model)
          builder.add_edge(START, "call_model")

          graph = builder.compile(
              checkpointer=checkpointer,
              store=store,
          )

          config = {
              "configurable": {
                  "thread_id": "1",
                  "user_id": "1",
              }
          }
          async for chunk in graph.astream(
              {"messages": [{"role": "user", "content": "Hi! Remember: my name is Bob"}]},
              config,
              stream_mode="values",
          ):
              chunk["messages"][-1].pretty_print()

          config = {
              "configurable": {
                  "thread_id": "2",
                  "user_id": "1",
              }
          }

          async for chunk in graph.astream(
              {"messages": [{"role": "user", "content": "what is my name?"}]},
              config,
              stream_mode="values",
          ):
              chunk["messages"][-1].pretty_print()
```
        </Tab>
    </Tabs>



</Accordion>

<Accordion title="Example: using Redis store">
  ```
  pip install -U langgraph langgraph-checkpoint-redis
  ```

    <Tip>
    You need to call `store.setup()` the first time you're using Redis store
    </Tip>

    <Tabs>
        <Tab title="Sync">
      ```python
      from langchain_core.runnables import RunnableConfig
      from langchain.chat_models import init_chat_model
      from langgraph.graph import StateGraph, MessagesState, START
      from langgraph.checkpoint.redis import RedisSaver
      from langgraph.store.redis import RedisStore
      from langgraph.store.base import BaseStore
      import uuid

      model = init_chat_model(model="claude-haiku-4-5-20251001")

      DB_URI = "redis://localhost:6379"

      with (
          RedisStore.from_conn_string(DB_URI) as store,
          RedisSaver.from_conn_string(DB_URI) as checkpointer,
      ):
          store.setup()
          checkpointer.setup()

          def call_model(
              state: MessagesState,
              config: RunnableConfig,
              *,
              store: BaseStore,
          ):
              user_id = config["configurable"]["user_id"]
              namespace = ("memories", user_id)
              memories = store.search(namespace, query=str(state["messages"][-1].content))
              info = "\n".join([d.value["data"] for d in memories])
              system_msg = f"You are a helpful assistant talking to the user. User info: {info}"

              # Store new memories if the user asks the model to remember
              last_message = state["messages"][-1]
              if "remember" in last_message.content.lower():
                  memory = "User name is Bob"
                  store.put(namespace, str(uuid.uuid4()), {"data": memory})

              response = model.invoke(
                  [{"role": "system", "content": system_msg}] + state["messages"]
              )
              return {"messages": response}

          builder = StateGraph(MessagesState)
          builder.add_node(call_model)
          builder.add_edge(START, "call_model")

          graph = builder.compile(
              checkpointer=checkpointer,
              store=store,
          )

          config = {
              "configurable": {
                  "thread_id": "1",
                  "user_id": "1",
              }
          }
          for chunk in graph.stream(
              {"messages": [{"role": "user", "content": "Hi! Remember: my name is Bob"}]},
              config,
              stream_mode="values",
          ):
              chunk["messages"][-1].pretty_print()

          config = {
              "configurable": {
                  "thread_id": "2",
                  "user_id": "1",
              }
          }

          for chunk in graph.stream(
              {"messages": [{"role": "user", "content": "what is my name?"}]},
              config,
              stream_mode="values",
          ):
              chunk["messages"][-1].pretty_print()
```
        </Tab>
        <Tab title="Async">
      ```python
      from langchain_core.runnables import RunnableConfig
      from langchain.chat_models import init_chat_model
      from langgraph.graph import StateGraph, MessagesState, START
      from langgraph.checkpoint.redis.aio import AsyncRedisSaver
      from langgraph.store.redis.aio import AsyncRedisStore
      from langgraph.store.base import BaseStore
      import uuid

      model = init_chat_model(model="claude-haiku-4-5-20251001")

      DB_URI = "redis://localhost:6379"

      async with (
          AsyncRedisStore.from_conn_string(DB_URI) as store,
          AsyncRedisSaver.from_conn_string(DB_URI) as checkpointer,
      ):
          # await store.setup()
          # await checkpointer.asetup()

          async def call_model(
              state: MessagesState,
              config: RunnableConfig,
              *,
              store: BaseStore,
          ):
              user_id = config["configurable"]["user_id"]
              namespace = ("memories", user_id)
              memories = await store.asearch(namespace, query=str(state["messages"][-1].content))
              info = "\n".join([d.value["data"] for d in memories])
              system_msg = f"You are a helpful assistant talking to the user. User info: {info}"

              # Store new memories if the user asks the model to remember
              last_message = state["messages"][-1]
              if "remember" in last_message.content.lower():
                  memory = "User name is Bob"
                  await store.aput(namespace, str(uuid.uuid4()), {"data": memory})

              response = await model.ainvoke(
                  [{"role": "system", "content": system_msg}] + state["messages"]
              )
              return {"messages": response}

          builder = StateGraph(MessagesState)
          builder.add_node(call_model)
          builder.add_edge(START, "call_model")

          graph = builder.compile(
              checkpointer=checkpointer,
              store=store,
          )

          config = {
              "configurable": {
                  "thread_id": "1",
                  "user_id": "1",
              }
          }
          async for chunk in graph.astream(
              {"messages": [{"role": "user", "content": "Hi! Remember: my name is Bob"}]},
              config,
              stream_mode="values",
          ):
              chunk["messages"][-1].pretty_print()

          config = {
              "configurable": {
                  "thread_id": "2",
                  "user_id": "1",
              }
          }

          async for chunk in graph.astream(
              {"messages": [{"role": "user", "content": "what is my name?"}]},
              config,
              stream_mode="values",
          ):
              chunk["messages"][-1].pretty_print()
```
        </Tab>
    </Tabs>



</Accordion>

### Use semantic search

Enable semantic search in your graph's memory store to let graph agents search for items by semantic similarity instead of exact matches.

```python
from langchain.embeddings import init_embeddings
from langgraph.store.memory import InMemoryStore

# Create store with semantic search enabled
embeddings = init_embeddings("openai:text-embedding-3-small")
store = InMemoryStore(
    index={
        "embed": embeddings,
        "dims": 1536,
    }
)

store.put(("user_123", "memories"), "1", {"text": "I love pizza"})
store.put(("user_123", "memories"), "2", {"text": "I am a plumber"})

items = store.search(
    ("user_123", "memories"), query="I'm hungry", limit=1
)
```




<Accordion title="Full example: long-term memory with semantic search">
    ```python

    from langchain.embeddings import init_embeddings
    from langchain.chat_models import init_chat_model
    from langgraph.store.base import BaseStore
    from langgraph.store.memory import InMemoryStore
    from langgraph.graph import START, MessagesState, StateGraph

    model = init_chat_model("gpt-4o-mini")

    # Create store with semantic search enabled
    embeddings = init_embeddings("openai:text-embedding-3-small")
    store = InMemoryStore(
        index={
            "embed": embeddings,
            "dims": 1536,
        }
    )

    store.put(("user_123", "memories"), "1", {"text": "I love pizza"})
    store.put(("user_123", "memories"), "2", {"text": "I am a plumber"})

    def chat(state, *, store: BaseStore):
        # Search based on user's last message
        items = store.search(
            ("user_123", "memories"), query=state["messages"][-1].content, limit=2
        )
        memories = "\n".join(item.value["text"] for item in items)
        memories = f"## Memories of user\n{memories}" if memories else ""
        response = model.invoke(
            [
                {"role": "system", "content": f"You are a helpful assistant.\n{memories}"},
                *state["messages"],
            ]
        )
        return {"messages": [response]}


    builder = StateGraph(MessagesState)
    builder.add_node(chat)
    builder.add_edge(START, "chat")
    graph = builder.compile(store=store)

    for message, metadata in graph.stream(
        input={"messages": [{"role": "user", "content": "I'm hungry"}]},
        stream_mode="messages",
    ):
        print(message.content, end="")
    ```



</Accordion>

---

## Prebuilt memory tools

**LangMem** is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the [LangMem documentation](https://langchain-ai.github.io/langmem/) for usage examples.

---

<Callout icon="pen-to-square" iconType="regular">
    [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/add-memory.mdx)
</Callout>
<Tip icon="terminal" iconType="regular">
    [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>
